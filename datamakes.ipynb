{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "693f0cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in! Access JWT: eyJ0eXAiOiJhdCtqd3QiLCJhbGciOiJFUzI1NksifQ.eyJzY29wZSI6ImNvbS5hdHByb3RvLmFjY2VzcyIsInN1YiI6ImRpZDpwbGM6enBsZzNpYmN5am5vZng2YTdqcWx1c3czIiwiaWF0IjoxNzQ2OTYzMTE5LCJleHAiOjE3NDY5NzAzMTksImF1ZCI6ImRpZDp3ZWI6bGVjY2ludW0udXMtd2VzdC5ob3N0LmJza3kubmV0d29yayJ9.U1B8M8w_Uz_vlzoanZtEjlU4aZpdNc-_8YRKqpZXp5fFMZrumyW5U3MtydMzmlJ98WMj6zQRoIhHLdyH3H2SXw\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "\n",
    "PDS_URL  = \"https://bsky.social\"            # make sure this matches where your account lives\n",
    "USERNAME = \"timmyjunchen.bsky.social\"        # include the ‚Äú.bsky.social‚Äù suffix\n",
    "PASSWORD = \"Abcd1234\" # use an App-Password if you have 2FA enabled\n",
    "API_BASE = f\"{PDS_URL}/xrpc/app.bsky.feed.searchPosts\"\n",
    "\n",
    "url     = f\"{PDS_URL}/xrpc/com.atproto.server.createSession\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\"identifier\": USERNAME, \"password\": PASSWORD}\n",
    "\n",
    "resp = requests.post(url, json=payload, headers=headers, timeout=10)\n",
    "\n",
    "data = resp.json()\n",
    "access_jwt = data[\"accessJwt\"]\n",
    "refresh_jwt = data.get(\"refreshJwt\")  # for refreshing later, if needed\n",
    "\n",
    "print(\"Logged in! Access JWT:\", access_jwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d553457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bluesky_posts(\n",
    "    jwt: str,\n",
    "    query: str,\n",
    "    limit: int = 25,\n",
    "    cursor: Optional[str] = None,\n",
    "    sort: str = \"latest\",\n",
    "    lang: Optional[str] = \"en\"\n",
    ") -> Tuple[List[Dict], Optional[str]]:\n",
    "    headers = {\"Authorization\": f\"Bearer {jwt}\"}\n",
    "    params = {\"q\": query, \"limit\": limit, \"sort\": sort}\n",
    "    if cursor:\n",
    "        params[\"cursor\"] = cursor\n",
    "    if lang:\n",
    "        params[\"lang\"] = lang\n",
    "\n",
    "    resp = requests.get(API_BASE, params=params, headers=headers, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    posts = []\n",
    "    for hit in data.get(\"posts\", []):\n",
    "        rec = hit\n",
    "        posts.append({\n",
    "            \"uri\":        rec[\"uri\"],\n",
    "            \"author\":     hit[\"author\"][\"handle\"],\n",
    "            \"text\":       rec[\"record\"].get(\"text\", \"\"),\n",
    "            \"created_at\": rec[\"record\"][\"createdAt\"],\n",
    "            \"reply_count\":   rec[\"replyCount\"],\n",
    "            \"like_count\":    rec[\"likeCount\"],\n",
    "            \"repost_count\":  rec[\"repostCount\"]\n",
    "        })\n",
    "\n",
    "    return posts, data.get(\"cursor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0fc013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "def scrape_twitter_cli(query: str, limit: int = 50) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Calls `twscrape search <query> --limit <limit>`,\n",
    "    parses JSON-per-line stdout into dicts.\n",
    "    \"\"\"\n",
    "    cmd = [\"twscrape\", \"search\", query, f\"--limit={str(limit)}\"]\n",
    "    completed = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    tweets = []\n",
    "    for line in completed.stdout.splitlines():\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        rec = json.loads(line)\n",
    "        rec.update({\n",
    "            \"platform\": \"twitter\",\n",
    "            \"query\":    query\n",
    "        })\n",
    "        tweets.append(rec)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e431860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def search_reddit_posts(query: str, date: str, limit: int = 20) -> List[Dict]:\n",
    "    if isinstance(date, str):\n",
    "        # parse ISO8601 with trailing Z\n",
    "        date = datetime.strptime(date, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "        client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "        user_agent=os.getenv(\"REDDIT_USER_AGENT\"),\n",
    "    )\n",
    "\n",
    "    matched = []\n",
    "    for submission in reddit.subreddit(\"all\").search(\n",
    "        query,\n",
    "        sort=\"top\",           # get top posts first\n",
    "        time_filter=\"all\",    # consider all time\n",
    "        limit=100           # fetch up to 100\n",
    "    ):\n",
    "        created = datetime.fromtimestamp(submission.created_utc)\n",
    "        if created < date:\n",
    "            matched.append({\n",
    "                \"title\":      submission.title,\n",
    "                \"text\":       submission.selftext,\n",
    "                \"upvotes\":    submission.score,\n",
    "                \"comments\":   submission.num_comments,\n",
    "                \"created_at\": created,\n",
    "            })\n",
    "            if len(matched) >= limit:\n",
    "                break  # we‚Äôve got our 20\n",
    "\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import re\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def build_training_dataset(\n",
    "    events_df: pd.DataFrame,\n",
    "    # twitter_limit: int,\n",
    "    bluesky_limit: int,\n",
    "    reddit_limit:int,\n",
    "    access_jwt: str,\n",
    "    max_retries: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    # empty = pd.DataFrame(columns=[f\"post_{i+1}\" for i in range(70)] + [\"result\",\"yes_bid\"])\n",
    "    # empty.to_csv(\"training_data.csv\", index=False)\n",
    "\n",
    "    num = 0\n",
    "    for _, ev in events_df.iterrows():\n",
    "        if num < 100:\n",
    "            num += 1\n",
    "            continue\n",
    "        # 1) gather raw texts\n",
    "        raw_texts = []\n",
    "\n",
    "        is_negated = bool(re.match(r\"(?i)^\\s*Not:\\s*\", ev[\"topic\"]))\n",
    "        is_season   = bool(re.search(r\"(?i)Season\\s*\\d+\", ev[\"topic\"]))\n",
    "\n",
    "        clean_topic = re.sub(r\"\\s*\\([^)]*\\)\", \"\", ev[\"topic\"])\n",
    "\n",
    "        #Remove Not... and negate result\n",
    "        clean_topic = re.sub(r\"(?i)^\\s*Not\\s*\", \"\", clean_topic)\n",
    "\n",
    "        #Remove ...:Season 1\n",
    "        clean_topic  = re.sub(r\"(?i):\\s*Season\\s*\\d+\", \"\", clean_topic).strip()\n",
    "\n",
    "        #Remove by 'artist'\n",
    "        clean_topic = re.sub(r\"\\s+by\\s+.*$\", \"\", clean_topic, flags=re.IGNORECASE)\n",
    "\n",
    "        if 'app' in clean_topic.lower() or is_season:\n",
    "            query = f\"{clean_topic} until:{ev['expiration_time']}\"\n",
    "        else:\n",
    "            query = f\"{clean_topic} song until:{ev['expiration_time']}\"\n",
    "        \n",
    "        print(f\"Iteration: {num}\")\n",
    "        print(query)\n",
    "        # ‚Äî Twitter ‚Äî\n",
    "        # try:\n",
    "        #     tw_recs = scrape_twitter_cli(query + \" lang:en\", limit=twitter_limit)\n",
    "        # except subprocess.CalledProcessError as e:\n",
    "        #     print(f\"Twitter scrape failed for {query}: {e}\")\n",
    "        #     tw_recs = []\n",
    "        # raw_texts += [r[\"rawContent\"] for r in tw_recs]\n",
    "        \n",
    "        # ‚Äî Bluesky ‚Äî\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                bs_recs, _ = search_bluesky_posts(access_jwt, query, limit=bluesky_limit)\n",
    "            except Exception as e:\n",
    "                print(f\"Bluesky error: {e}\")\n",
    "                bs_recs = []\n",
    "            if bs_recs:\n",
    "                break\n",
    "            retries += 1\n",
    "            print(f\"  ‚Üí Bluesky empty, retry {retries}/{max_retries} in 10 s\")\n",
    "            time.sleep(10)\n",
    "\n",
    "        raw_texts += [r[\"text\"] for r in bs_recs]\n",
    "        \n",
    "        # ‚Äî Reddit ‚Äî\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                rd_recs = search_reddit_posts(query, ev.expiration_time, limit=reddit_limit)\n",
    "            except Exception as e:\n",
    "                print(f\"Reddit error: {e}\")\n",
    "                rd_recs = []\n",
    "            if rd_recs:\n",
    "                break\n",
    "            retries += 1\n",
    "            print(f\"  ‚Üí Reddit empty, retry {retries}/{max_retries} in 10 s\")\n",
    "            time.sleep(10)\n",
    "        raw_texts += [\n",
    "            (r[\"title\"] + \"\\n\\n\" + r[\"text\"]).strip()\n",
    "            for r in rd_recs\n",
    "        ]\n",
    "        \n",
    "        # 2) compute compound sentiment scores\n",
    "        compounds = [analyzer.polarity_scores(txt)[\"compound\"] \n",
    "                     for txt in raw_texts]\n",
    "        \n",
    "        print(f\"Num of posts: {len(compounds)}; Bluesky: {len(bs_recs)}; Reddit: {len(rd_recs)}\")\n",
    "        \n",
    "        orig_result = ev[\"result\"]\n",
    "        if is_negated:\n",
    "            # assuming 'yes'/'no' strings; adjust if yours are booleans or other labels\n",
    "            flipped = \"no\" if str(orig_result).lower() == \"yes\" else \"yes\"\n",
    "        else:\n",
    "            flipped = orig_result\n",
    "        \n",
    "        row_df = pd.DataFrame([{\n",
    "            **{f\"post_{i+1}\": compounds[i] for i in range(len(compounds))},\n",
    "            \"result\": flipped,\n",
    "            \"yes_bid\": ev[\"yes_bid\"]\n",
    "        }])\n",
    "        \n",
    "        row_df.to_csv(\"training_data.csv\", mode=\"a\", header=False, index=False)\n",
    "        num += 1\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "KALSHI_EVENTS = \"https://api.elections.kalshi.com/trade-api/v2/events\"\n",
    "culture_series = [\"KXTOPSONG\", \"KXNETFLIXRANKSHOW\", \"KXAPPRANKFREE\"] #KXSPOTIFYD\n",
    "events = []\n",
    "for ticker in culture_series:\n",
    "    cursor = None\n",
    "    while True:\n",
    "        params = dict(\n",
    "            limit=200,\n",
    "            cursor=cursor,\n",
    "            series_ticker=ticker,\n",
    "            with_nested_markets=True\n",
    "        )\n",
    "        payload = requests.get(KALSHI_EVENTS, params=params).json()\n",
    "        for ev in payload.get(\"events\", []):\n",
    "            for market in ev[\"markets\"]:\n",
    "                events.append({\n",
    "                    \"series\":      ticker,\n",
    "                    \"title\":       ev[\"title\"],\n",
    "                    \"topic\":  market[\"no_sub_title\"],\n",
    "                    \"result\": market[\"result\"],\n",
    "                    \"expiration_time\": market[\"expiration_time\"],\n",
    "                    \"yes_bid\": market[\"yes_bid\"],\n",
    "                    \"category\": market[\"category\"],\n",
    "                })\n",
    "        cursor = payload.get(\"cursor\")\n",
    "        if not cursor:\n",
    "            break\n",
    "\n",
    "events_df = pd.DataFrame(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7a5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "      <th>result</th>\n",
       "      <th>expiration_time</th>\n",
       "      <th>yes_bid</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KXTOPSONG</td>\n",
       "      <td>Billboard Hot 100 #1 on May 24, 2025 chart?</td>\n",
       "      <td>Ordinary</td>\n",
       "      <td></td>\n",
       "      <td>2025-06-07T14:00:00Z</td>\n",
       "      <td>43</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KXTOPSONG</td>\n",
       "      <td>Billboard Hot 100 #1 on May 24, 2025 chart?</td>\n",
       "      <td>Luther</td>\n",
       "      <td></td>\n",
       "      <td>2025-06-07T14:00:00Z</td>\n",
       "      <td>41</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KXTOPSONG</td>\n",
       "      <td>Billboard Hot 100 #1 on May 17, 2025 chart?</td>\n",
       "      <td>luther</td>\n",
       "      <td></td>\n",
       "      <td>2025-05-31T14:00:00Z</td>\n",
       "      <td>94</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>KXTOPSONG</td>\n",
       "      <td>Billboard Hot 100 #1 on Apr 26, 2025 chart?</td>\n",
       "      <td>luther</td>\n",
       "      <td>yes</td>\n",
       "      <td>2025-05-10T14:00:00Z</td>\n",
       "      <td>97</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>KXTOPSONG</td>\n",
       "      <td>Billboard Hot 100 #1 on Apr 19, 2025 chart?</td>\n",
       "      <td>luther</td>\n",
       "      <td>yes</td>\n",
       "      <td>2025-05-03T14:00:00Z</td>\n",
       "      <td>99</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>KXTOPSONG</td>\n",
       "      <td>Billboard Hot 100 #1 on Jan 27, 2024?</td>\n",
       "      <td>Not Lovin on Me</td>\n",
       "      <td>no</td>\n",
       "      <td>2024-02-10T15:00:00Z</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>KXTOPSONG</td>\n",
       "      <td>Billboard Hot 100 #1 on Jan 20, 2024?</td>\n",
       "      <td>Not Lovin On Me</td>\n",
       "      <td>yes</td>\n",
       "      <td>2024-02-03T15:00:00Z</td>\n",
       "      <td>95</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>KXTOPSONG</td>\n",
       "      <td>Billboard Hot 100 #1 on Jan 13, 2024?</td>\n",
       "      <td>Not Lovin On Me</td>\n",
       "      <td>yes</td>\n",
       "      <td>2024-01-27T15:00:00Z</td>\n",
       "      <td>97</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>KXTOPSONG</td>\n",
       "      <td>Billboard Hot 100 #1 on Jan 6, 2024?</td>\n",
       "      <td>Not Rockin' Around The Christmas Tree</td>\n",
       "      <td>yes</td>\n",
       "      <td>2024-01-20T15:00:00Z</td>\n",
       "      <td>80</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>KXTOPSONG</td>\n",
       "      <td>Billboard Hot 100 #1 on Dec 30, 2023?</td>\n",
       "      <td>Not All I Want for Christmas is You</td>\n",
       "      <td>yes</td>\n",
       "      <td>2024-01-13T15:00:00Z</td>\n",
       "      <td>92</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        series                                        title  \\\n",
       "5    KXTOPSONG  Billboard Hot 100 #1 on May 24, 2025 chart?   \n",
       "7    KXTOPSONG  Billboard Hot 100 #1 on May 24, 2025 chart?   \n",
       "13   KXTOPSONG  Billboard Hot 100 #1 on May 17, 2025 chart?   \n",
       "51   KXTOPSONG  Billboard Hot 100 #1 on Apr 26, 2025 chart?   \n",
       "57   KXTOPSONG  Billboard Hot 100 #1 on Apr 19, 2025 chart?   \n",
       "..         ...                                          ...   \n",
       "482  KXTOPSONG        Billboard Hot 100 #1 on Jan 27, 2024?   \n",
       "490  KXTOPSONG        Billboard Hot 100 #1 on Jan 20, 2024?   \n",
       "495  KXTOPSONG        Billboard Hot 100 #1 on Jan 13, 2024?   \n",
       "498  KXTOPSONG         Billboard Hot 100 #1 on Jan 6, 2024?   \n",
       "511  KXTOPSONG        Billboard Hot 100 #1 on Dec 30, 2023?   \n",
       "\n",
       "                                     topic result       expiration_time  \\\n",
       "5                                 Ordinary         2025-06-07T14:00:00Z   \n",
       "7                                   Luther         2025-06-07T14:00:00Z   \n",
       "13                                  luther         2025-05-31T14:00:00Z   \n",
       "51                                  luther    yes  2025-05-10T14:00:00Z   \n",
       "57                                  luther    yes  2025-05-03T14:00:00Z   \n",
       "..                                     ...    ...                   ...   \n",
       "482                        Not Lovin on Me     no  2024-02-10T15:00:00Z   \n",
       "490                        Not Lovin On Me    yes  2024-02-03T15:00:00Z   \n",
       "495                        Not Lovin On Me    yes  2024-01-27T15:00:00Z   \n",
       "498  Not Rockin' Around The Christmas Tree    yes  2024-01-20T15:00:00Z   \n",
       "511    Not All I Want for Christmas is You    yes  2024-01-13T15:00:00Z   \n",
       "\n",
       "     yes_bid category  \n",
       "5         43           \n",
       "7         41           \n",
       "13        94           \n",
       "51        97           \n",
       "57        99           \n",
       "..       ...      ...  \n",
       "482        1           \n",
       "490       95           \n",
       "495       97           \n",
       "498       80           \n",
       "511       92           \n",
       "\n",
       "[95 rows x 7 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df = events_df.iloc[::-1]\n",
    "events_df = events_df[events_df[\"yes_bid\"] > 0]\n",
    "events_df.head(83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10de04ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'query' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset_df = \u001b[43mbuild_training_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevents_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# twitter_limit=20,\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbluesky_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreddit_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccess_jwt\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccess_jwt\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mbuild_training_dataset\u001b[39m\u001b[34m(events_df, bluesky_limit, reddit_limit, access_jwt, max_retries)\u001b[39m\n\u001b[32m     35\u001b[39m clean_topic  = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(?i):\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*Season\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, clean_topic).strip()\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m#Remove by 'artist'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m clean_topic = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+by\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+.*$\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m, query, flags=re.IGNORECASE)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mapp\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m clean_topic.lower() \u001b[38;5;129;01mor\u001b[39;00m is_season:\n\u001b[32m     41\u001b[39m     query = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_topic\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m until:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mev[\u001b[33m'\u001b[39m\u001b[33mexpiration_time\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'query' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "dataset_df = build_training_dataset(\n",
    "        events_df,\n",
    "        # twitter_limit=20,\n",
    "        bluesky_limit=20,\n",
    "        reddit_limit=20,\n",
    "        access_jwt=access_jwt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0b39b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_1</th>\n",
       "      <th>post_2</th>\n",
       "      <th>post_3</th>\n",
       "      <th>post_4</th>\n",
       "      <th>post_5</th>\n",
       "      <th>post_6</th>\n",
       "      <th>post_7</th>\n",
       "      <th>post_8</th>\n",
       "      <th>post_9</th>\n",
       "      <th>post_10</th>\n",
       "      <th>...</th>\n",
       "      <th>post_51</th>\n",
       "      <th>post_52</th>\n",
       "      <th>post_53</th>\n",
       "      <th>post_54</th>\n",
       "      <th>post_55</th>\n",
       "      <th>post_56</th>\n",
       "      <th>post_57</th>\n",
       "      <th>post_58</th>\n",
       "      <th>post_59</th>\n",
       "      <th>post_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welcome to life St Chroma!</td>\n",
       "      <td>Welcome to the world SPECIALZ https://t.co/B2Z...</td>\n",
       "      <td>@guicosz st chroma no B üíÄ</td>\n",
       "      <td>@tlprkive st. chroma</td>\n",
       "      <td>@Glory_be_Satori mf rose from the dead</td>\n",
       "      <td>damn itft is really good</td>\n",
       "      <td>@tylerthecreator as i navigate my own life and...</td>\n",
       "      <td>@tylerthecreator st chroma - with music i base...</td>\n",
       "      <td>@VampireGhuleh Unironically listen to St. Chro...</td>\n",
       "      <td>@tylerthecreator the beats transitions instrum...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@tylerthecreator could you get someone to get ...</td>\n",
       "      <td>i still wanna do a felix darling i edit.. but ...</td>\n",
       "      <td>@allishastlouis Ain't no way boy https://t.co/...</td>\n",
       "      <td>St. Chroma is truly incredible man. Tyler can ...</td>\n",
       "      <td>umm‚Ä¶‚Ä¶ yay? https://t.co/ITgyOi2dm3</td>\n",
       "      <td>@beaches_p Weezer favs: Undone - The Sweater S...</td>\n",
       "      <td>@cutegashu congrats now look at this funny guy...</td>\n",
       "      <td>MISS MY MO+LI</td>\n",
       "      <td>@immaleakyoshit they're correct</td>\n",
       "      <td>@fazrabbit2 @Cosmicnoone Are you stupid it say...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No matter what happens tomorrow, it'll still b...</td>\n",
       "      <td>@Jasp3r_0 I just thinks that st. Chroma sounds...</td>\n",
       "      <td>tight\\n\\nbut St. Chroma is better</td>\n",
       "      <td>Stars vs St. Chroma \\n\\nWhat is the better int...</td>\n",
       "      <td>Idk why but the song st. Chroma makes me want ...</td>\n",
       "      <td>@satosugaru okay but sticky goes soooo hard to...</td>\n",
       "      <td>@Jasp3r_0 St. Chroma</td>\n",
       "      <td>@bbokariswingz that‚Äôs a bit üè≥Ô∏è‚Äçüåà</td>\n",
       "      <td>Close but St. Chroma is better.</td>\n",
       "      <td>@cutegashu gashu what demons were you fighting</td>\n",
       "      <td>...</td>\n",
       "      <td>My friend and I are working on a film about th...</td>\n",
       "      <td>St Chroma walk cycle</td>\n",
       "      <td>I need St. Chroma injected into my veins</td>\n",
       "      <td>ST chroma</td>\n",
       "      <td>I already knew St. Chroma was gonna be my fav ...</td>\n",
       "      <td>1. ST CHROMA\\n\\n#furry #furryvr #vrchat #furry...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Cruela_DeVil_ Yeah I opened it all and realis...</td>\n",
       "      <td>@TeTheGamer It‚Äôs $jam \\nI feel it in my balls</td>\n",
       "      <td>üé≠ Akool vs. Synthesia\\n\\nWhen it comes to AI A...</td>\n",
       "      <td>I really like playing mercy</td>\n",
       "      <td>Shitty sticky notes shouldn‚Äôt be on the market...</td>\n",
       "      <td>@cookitup31 Link broke already ü§¶üèæ‚Äç‚ôÇÔ∏è</td>\n",
       "      <td>@ape_descendent Dust sprayer and sticky tape</td>\n",
       "      <td>@larryelder Transitory over 3 years. Hmmm‚Ä¶..Th...</td>\n",
       "      <td>@jadelaui Sure, but I have to warn you, there‚Äô...</td>\n",
       "      <td>@cmonJD stop why did we both tweet about mingy...</td>\n",
       "      <td>...</td>\n",
       "      <td>The original choreographer of the viral Sticky...</td>\n",
       "      <td>Ï†úÌîºPure Zephyrüê∫üåõ (üîûVtuber/ASMRtist) - „Äê V - Zep...</td>\n",
       "      <td>slaps this sticky note to your big ass forehead</td>\n",
       "      <td>It did a great job of creating a really tense ...</td>\n",
       "      <td>Better find a mop , it‚Äôs getting sticky in thi...</td>\n",
       "      <td>Some little guys I was sketching for a creatur...</td>\n",
       "      <td>Woke up and EXTREMELY horny. This fox alreedy ...</td>\n",
       "      <td>Oh I‚Äôm a cold and frosty lover too! Give me th...</td>\n",
       "      <td>Nov 14, 2024\\nWild Provisions Beer Project: 3p...</td>\n",
       "      <td>Glo‚Äôs verse on sticky &gt;&gt;&gt;&gt;&gt;&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              post_1  \\\n",
       "0                         Welcome to life St Chroma!   \n",
       "1  @tylerthecreator could you get someone to get ...   \n",
       "2  No matter what happens tomorrow, it'll still b...   \n",
       "3  @Cruela_DeVil_ Yeah I opened it all and realis...   \n",
       "\n",
       "                                              post_2  \\\n",
       "0  Welcome to the world SPECIALZ https://t.co/B2Z...   \n",
       "1  i still wanna do a felix darling i edit.. but ...   \n",
       "2  @Jasp3r_0 I just thinks that st. Chroma sounds...   \n",
       "3      @TeTheGamer It‚Äôs $jam \\nI feel it in my balls   \n",
       "\n",
       "                                              post_3  \\\n",
       "0                          @guicosz st chroma no B üíÄ   \n",
       "1  @allishastlouis Ain't no way boy https://t.co/...   \n",
       "2                  tight\\n\\nbut St. Chroma is better   \n",
       "3  üé≠ Akool vs. Synthesia\\n\\nWhen it comes to AI A...   \n",
       "\n",
       "                                              post_4  \\\n",
       "0                               @tlprkive st. chroma   \n",
       "1  St. Chroma is truly incredible man. Tyler can ...   \n",
       "2  Stars vs St. Chroma \\n\\nWhat is the better int...   \n",
       "3                        I really like playing mercy   \n",
       "\n",
       "                                              post_5  \\\n",
       "0             @Glory_be_Satori mf rose from the dead   \n",
       "1                 umm‚Ä¶‚Ä¶ yay? https://t.co/ITgyOi2dm3   \n",
       "2  Idk why but the song st. Chroma makes me want ...   \n",
       "3  Shitty sticky notes shouldn‚Äôt be on the market...   \n",
       "\n",
       "                                              post_6  \\\n",
       "0                           damn itft is really good   \n",
       "1  @beaches_p Weezer favs: Undone - The Sweater S...   \n",
       "2  @satosugaru okay but sticky goes soooo hard to...   \n",
       "3               @cookitup31 Link broke already ü§¶üèæ‚Äç‚ôÇÔ∏è   \n",
       "\n",
       "                                              post_7  \\\n",
       "0  @tylerthecreator as i navigate my own life and...   \n",
       "1  @cutegashu congrats now look at this funny guy...   \n",
       "2                               @Jasp3r_0 St. Chroma   \n",
       "3       @ape_descendent Dust sprayer and sticky tape   \n",
       "\n",
       "                                              post_8  \\\n",
       "0  @tylerthecreator st chroma - with music i base...   \n",
       "1                                      MISS MY MO+LI   \n",
       "2                   @bbokariswingz that‚Äôs a bit üè≥Ô∏è‚Äçüåà   \n",
       "3  @larryelder Transitory over 3 years. Hmmm‚Ä¶..Th...   \n",
       "\n",
       "                                              post_9  \\\n",
       "0  @VampireGhuleh Unironically listen to St. Chro...   \n",
       "1                    @immaleakyoshit they're correct   \n",
       "2                    Close but St. Chroma is better.   \n",
       "3  @jadelaui Sure, but I have to warn you, there‚Äô...   \n",
       "\n",
       "                                             post_10  ...  \\\n",
       "0  @tylerthecreator the beats transitions instrum...  ...   \n",
       "1  @fazrabbit2 @Cosmicnoone Are you stupid it say...  ...   \n",
       "2     @cutegashu gashu what demons were you fighting  ...   \n",
       "3  @cmonJD stop why did we both tweet about mingy...  ...   \n",
       "\n",
       "                                             post_51  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  My friend and I are working on a film about th...   \n",
       "3  The original choreographer of the viral Sticky...   \n",
       "\n",
       "                                             post_52  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                               St Chroma walk cycle   \n",
       "3  Ï†úÌîºPure Zephyrüê∫üåõ (üîûVtuber/ASMRtist) - „Äê V - Zep...   \n",
       "\n",
       "                                           post_53  \\\n",
       "0                                              NaN   \n",
       "1                                              NaN   \n",
       "2         I need St. Chroma injected into my veins   \n",
       "3  slaps this sticky note to your big ass forehead   \n",
       "\n",
       "                                             post_54  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                          ST chroma   \n",
       "3  It did a great job of creating a really tense ...   \n",
       "\n",
       "                                             post_55  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  I already knew St. Chroma was gonna be my fav ...   \n",
       "3  Better find a mop , it‚Äôs getting sticky in thi...   \n",
       "\n",
       "                                             post_56  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  1. ST CHROMA\\n\\n#furry #furryvr #vrchat #furry...   \n",
       "3  Some little guys I was sketching for a creatur...   \n",
       "\n",
       "                                             post_57  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  Woke up and EXTREMELY horny. This fox alreedy ...   \n",
       "\n",
       "                                             post_58  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  Oh I‚Äôm a cold and frosty lover too! Give me th...   \n",
       "\n",
       "                                             post_59  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  Nov 14, 2024\\nWild Provisions Beer Project: 3p...   \n",
       "\n",
       "                        post_60  \n",
       "0                           NaN  \n",
       "1                           NaN  \n",
       "2                           NaN  \n",
       "3  Glo‚Äôs verse on sticky >>>>>>  \n",
       "\n",
       "[4 rows x 60 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "345b3f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timmy\\AppData\\Local\\Temp\\ipykernel_47684\\3433543020.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  compound_df = texts.applymap(lambda txt: analyzer.polarity_scores(txt)['compound'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_1</th>\n",
       "      <th>post_2</th>\n",
       "      <th>post_3</th>\n",
       "      <th>post_4</th>\n",
       "      <th>post_5</th>\n",
       "      <th>post_6</th>\n",
       "      <th>post_7</th>\n",
       "      <th>post_8</th>\n",
       "      <th>post_9</th>\n",
       "      <th>post_10</th>\n",
       "      <th>...</th>\n",
       "      <th>post_51</th>\n",
       "      <th>post_52</th>\n",
       "      <th>post_53</th>\n",
       "      <th>post_54</th>\n",
       "      <th>post_55</th>\n",
       "      <th>post_56</th>\n",
       "      <th>post_57</th>\n",
       "      <th>post_58</th>\n",
       "      <th>post_59</th>\n",
       "      <th>post_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5093</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.6486</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.2585</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.3412</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>-0.2519</td>\n",
       "      <td>0.7766</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.5267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.0191</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>0.5927</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>0.7684</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5927</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7263</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.1635</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>0.7674</td>\n",
       "      <td>-0.8020</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>-0.6808</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>0.8682</td>\n",
       "      <td>-0.4389</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_1  post_2  post_3  post_4  post_5  post_6  post_7  post_8  post_9  \\\n",
       "0  0.5093  0.4588 -0.2960  0.0000 -0.6486  0.1263  0.8126  0.2585  0.0000   \n",
       "1 -0.3412  0.3400  0.2235  0.4404  0.5267 -0.2519  0.7766 -0.1531  0.0000   \n",
       "2 -0.0191 -0.4019  0.5927  0.4404  0.5859  0.7684  0.0000  0.0000  0.5927   \n",
       "3 -0.1635  0.0000  0.7569  0.7674 -0.8020 -0.4215  0.0000  0.0000  0.0900   \n",
       "\n",
       "   post_10  ...  post_51  post_52  post_53  post_54  post_55  post_56  \\\n",
       "0   0.7086  ...   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   \n",
       "1  -0.5267  ...   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   \n",
       "2  -0.3612  ...   0.7263   0.0000   0.0000   0.0000   0.2500   0.0000   \n",
       "3  -0.2960  ...   0.6249  -0.6808  -0.5423   0.8682  -0.4389   0.1007   \n",
       "\n",
       "   post_57  post_58  post_59  post_60  \n",
       "0      0.0   0.0000      0.0      0.0  \n",
       "1      0.0   0.0000      0.0      0.0  \n",
       "2      0.0   0.0000      0.0      0.0  \n",
       "3      0.0   0.6239      0.0      0.0  \n",
       "\n",
       "[4 rows x 60 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import json\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "texts = dataset_df.astype(str)\n",
    "\n",
    "# 2) applymap over every cell, extracting only 'compound'\n",
    "compound_df = texts.applymap(lambda txt: analyzer.polarity_scores(txt)['compound'])\n",
    "compound_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70ef3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_df[\"result\"] = events_df[\"result\"][:4].values\n",
    "compound_df[\"yes_bid\"] = events_df[\"yes_bid\"][:4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "27ee345c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_1</th>\n",
       "      <th>post_2</th>\n",
       "      <th>post_3</th>\n",
       "      <th>post_4</th>\n",
       "      <th>post_5</th>\n",
       "      <th>post_6</th>\n",
       "      <th>post_7</th>\n",
       "      <th>post_8</th>\n",
       "      <th>post_9</th>\n",
       "      <th>post_10</th>\n",
       "      <th>...</th>\n",
       "      <th>post_53</th>\n",
       "      <th>post_54</th>\n",
       "      <th>post_55</th>\n",
       "      <th>post_56</th>\n",
       "      <th>post_57</th>\n",
       "      <th>post_58</th>\n",
       "      <th>post_59</th>\n",
       "      <th>post_60</th>\n",
       "      <th>result</th>\n",
       "      <th>yes_bid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5093</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.6486</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0.2585</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.3412</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>-0.2519</td>\n",
       "      <td>0.7766</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.5267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.0191</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>0.5927</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>0.7684</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5927</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.1635</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>0.7674</td>\n",
       "      <td>-0.8020</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>0.8682</td>\n",
       "      <td>-0.4389</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_1  post_2  post_3  post_4  post_5  post_6  post_7  post_8  post_9  \\\n",
       "0  0.5093  0.4588 -0.2960  0.0000 -0.6486  0.1263  0.8126  0.2585  0.0000   \n",
       "1 -0.3412  0.3400  0.2235  0.4404  0.5267 -0.2519  0.7766 -0.1531  0.0000   \n",
       "2 -0.0191 -0.4019  0.5927  0.4404  0.5859  0.7684  0.0000  0.0000  0.5927   \n",
       "3 -0.1635  0.0000  0.7569  0.7674 -0.8020 -0.4215  0.0000  0.0000  0.0900   \n",
       "\n",
       "   post_10  ...  post_53  post_54  post_55  post_56  post_57  post_58  \\\n",
       "0   0.7086  ...   0.0000   0.0000   0.0000   0.0000      0.0   0.0000   \n",
       "1  -0.5267  ...   0.0000   0.0000   0.0000   0.0000      0.0   0.0000   \n",
       "2  -0.3612  ...   0.0000   0.0000   0.2500   0.0000      0.0   0.0000   \n",
       "3  -0.2960  ...  -0.5423   0.8682  -0.4389   0.1007      0.0   0.6239   \n",
       "\n",
       "   post_59  post_60  result  yes_bid  \n",
       "0      0.0      0.0     yes       95  \n",
       "1      0.0      0.0     yes       99  \n",
       "2      0.0      0.0      no       70  \n",
       "3      0.0      0.0      no       10  \n",
       "\n",
       "[4 rows x 62 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compound_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96100f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_df.to_csv(\"training_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35d2aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_csv  = \"training_data.csv\"\n",
    "output_csv = \"training_data_reordered.csv\"\n",
    "\n",
    "with open(input_csv,  newline=\"\", encoding=\"utf-8\") as fin, \\\n",
    "     open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "    \n",
    "    reader = csv.reader(fin)\n",
    "    writer = csv.writer(fout)\n",
    "\n",
    "    # 1) Reorder the header row\n",
    "    header      = next(reader)\n",
    "    new_header  = header[-2:] + header[:-2]\n",
    "    writer.writerow(new_header)\n",
    "\n",
    "    # 2) For each data row, move its last two fields to the front\n",
    "    for row in reader:\n",
    "        if len(row) >= 2:\n",
    "            new_row = row[-2:] + row[:-2]\n",
    "        else:\n",
    "            # edge case: fewer than 2 fields‚Äîjust write as-is\n",
    "            new_row = row\n",
    "        writer.writerow(new_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
